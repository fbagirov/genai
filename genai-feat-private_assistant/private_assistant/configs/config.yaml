llm:
  provider: ollama
  model: llama3:instruct
  temperature: 0.2
  max_new_tokens: 512

safety:
  guardrails: true              # apply policies/guardrails.yaml
  pii_scrub: false              # scrub PII via Presidio pre/post stages


privacy:
  telemetry: false
  persist_history: false
  log_requests: false

server:
  host: 127.0.0.1
  port: 8000

base_model_id: TinyLlama/TinyLlama-1.1B-Chat-v1.0
tokenizer_id: null
output_dir: outputs

quantization:
  load_in_4bit: false
  bnb_4bit_compute_dtype: bfloat16
  bnb_4bit_quant_type: nf4
  bnb_4bit_use_double_quant: true

lora:
  r: 16
  alpha: 32
  dropout: 0.05
  target_modules: auto

tracking:
  mlflow_tracking_uri: mlruns

sft:
  # dataset_path: data/dpo/sample_dpo.jsonl 
  dataset_path: data/sft/mydata.jsonl
  max_seq_len: 256   # start smaller for 1â€“10 examples
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 4
  num_train_epochs: 1
  learning_rate: 2e-4
  weight_decay: 0.0
  warmup_ratio: 0.03
  logging_steps: 10
  save_steps: 1000
  # max_seq_len: 512
  # per_device_train_batch_size: 2
  # gradient_accumulation_steps: 4
  # num_train_epochs: 1
  # learning_rate: 2e-4
  # weight_decay: 0.0
  # warmup_ratio: 0.03
  # logging_steps: 10
  # save_steps: 1000
  adapter_out: outputs/sft_adapter

dpo:
  # dataset_path: data/dpo/sample_dpo.jsonl 
  dataset_path: data/dpo/sample_dpo.jsonl
  max_length: 512
  max_prompt_length: 256
  beta: 0.1
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 4
  num_train_epochs: 1
  learning_rate: 2e-5
  logging_steps: 10
  save_steps: 1000
  init_from_sft: true
  adapter_out: outputs/dpo_adapter

serve:
  adapter_path: outputs/dpo_adapter
  max_new_tokens: 256
  temperature: 0.2
